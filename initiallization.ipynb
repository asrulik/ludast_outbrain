{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "import zipfile\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#directory path\n",
    "path = os.getcwd() + \"/\"\n",
    "path_t = path + \"source_tables/\"\n",
    "path_b = path_t + \"built/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 for activating, 0 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unzip the tables - *must run at first use*\n",
    "unzip = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*must run at first use*\n",
    "#prepare events\n",
    "prep_events = 0\n",
    "\n",
    "#prepare topics and categories\n",
    "prep_topics_categories = 0\n",
    "\n",
    "#prepare promoted content adding topics and categories\n",
    "prep_promoted_content = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optional, these files are already in the zips\n",
    "#build all the ctr tables\n",
    "build_ctr = 0\n",
    "\n",
    "#build the timestamp table with offsets by geographic location\n",
    "build_time = 0\n",
    "#import and use shapely\n",
    "shapley_f = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promoted_content_prep.csv created in path/source_tables/built/ directory\n"
     ]
    }
   ],
   "source": [
    "#unzip\n",
    "if unzip:\n",
    "    zip_ref = zipfile.ZipFile(path_t + \"tables.zip\", 'r')\n",
    "    zip_ref.extractall(path_t)\n",
    "    zip_ref.close()\n",
    "    zip_ref = zipfile.ZipFile(path_b + \"built_tables.zip\", 'r')\n",
    "    zip_ref.extractall(path_b)\n",
    "    zip_ref.close()\n",
    "    print \"done unzipping\"\n",
    "    \n",
    "    \n",
    "if build_ctr:\n",
    "    #create all the CTR tables to load later, once created no need to recreate\n",
    "    ctr_features = path + \"ctr_features.py\"\n",
    "    %run $ctr_features\n",
    "    \n",
    "    \n",
    "if build_time:\n",
    "    if shapley_f:\n",
    "        for line in fileinput.input([path + \"timezone.py\", path + \"notebooks/timezone.ipynb\"], inplace = 1): \n",
    "            print line.replace(\"shapely_f = 0\", \"shapely_f = 1\").rstrip()\n",
    "    else:\n",
    "        for line in fileinput.input([path + \"timezone.py\", path + \"notebooks/timezone.ipynb\"], inplace = 1): \n",
    "            print line.replace(\"shapely_f = 1\", \"shapely_f = 0\").rstrip()\n",
    "    #create the table of timestamps corrected by the offset got by geographic location, once created no need to recreate\n",
    "    timezone = path + \"timezone.py\"\n",
    "    %run $timezone\n",
    "\n",
    "    \n",
    "if prep_events:\n",
    "    events = p.read_csv(path_b + \"events.csv\", usecols = [\"display_id\", \"document_id\", \"platform\"], dtype={\"display_id\":int, \"document_id\":int, \"platform\":str})\n",
    "    \n",
    "    #cast all platform to int, 5 missing values, changed to median\n",
    "    events.loc[events['platform'] == '\\\\N', 'platform'] = events['platform'][events['platform'] != '\\\\N'].median()\n",
    "    events['platform'] = events['platform'].apply(int)\n",
    "    \n",
    "    #prepare one-hot for platform\n",
    "    platform_dummies = p.get_dummies(events['platform'],prefix=\"plat\").astype(int)\n",
    "    events = events.drop('platform',axis = 1).join(platform_dummies)\n",
    "    del platform_dummies\n",
    "    \n",
    "    events.to_csv(path_b + 'events_prep.csv', index=False)\n",
    "    print \"events_prep.csv created in path/source_tables/built/ directory\"\n",
    "    \n",
    "\n",
    "if prep_topics_categories:\n",
    "    documents_topics = p.read_csv(path_t + \"documents_topics.csv\", dtype = {\"document_id\":int, \"topic_id\":int, \"confidence_level\":float})\n",
    "    documents_categories = p.read_csv(path_t + \"documents_categories.csv\", dtype = {\"document_id\":int, \"category_id\":int, \"confidence_level\":float})\n",
    "    \n",
    "    #rename columns of repeating names\n",
    "    documents_topics.rename(columns={'confidence_level': 'confi_top'}, inplace=True)\n",
    "    documents_categories.rename(columns={'confidence_level': 'confi_cat'}, inplace=True)\n",
    "    \n",
    "    #get only highest confidence topic and category\n",
    "    max_rows = documents_categories.groupby(['document_id'])['confi_cat'].transform(max) == documents_categories['confi_cat']\n",
    "    documents_categories = documents_categories[max_rows].drop_duplicates(subset = ['document_id','confi_cat'],keep = 'last')\n",
    "    max_rows = documents_topics.groupby(['document_id'])['confi_top'].transform(max) == documents_topics['confi_top']\n",
    "    documents_topics = documents_topics[max_rows].drop_duplicates(subset = ['document_id','confi_top'],keep = 'last')\n",
    "\n",
    "    #create one table for both categories and topics\n",
    "    topics_categories = documents_topics.merge(documents_categories, how = 'outer', on = 'document_id')\n",
    "    del max_rows, documents_categories, documents_topics\n",
    "    \n",
    "    #assign nulls to -1 and cast the ids back to int (the merge changes them to float)\n",
    "    topics_categories[['confi_cat', 'confi_top']] = topics_categories[['confi_cat', 'confi_top']].fillna(0)\n",
    "    topics_categories = topics_categories.fillna(-1)\n",
    "    topics_categories[['document_id', 'topic_id', 'category_id']] = topics_categories[['document_id', 'topic_id', 'category_id']].astype(int)\n",
    "    \n",
    "    topics_categories.to_csv(path_b + 'topics_categories.csv', index=False)\n",
    "    print \"topics_categories.csv created in path/source_tables/built/ directory\"\n",
    "    \n",
    "\n",
    "if prep_promoted_content:\n",
    "    promoted = p.read_csv(path_t + \"promoted_content.csv\", dtype = {\"ad_id\":int, \"document_id\":int, \"campaign_id\":int, \"advertiser_id\":int})\n",
    "    if not prep_topics_categories:\n",
    "        topics_categories = p.read_csv(path_b + \"topics_categories.csv\", dtype={\"document_id\":int, \"topic_id\":int, \"confi_top\":float, \"category_id\":int, \"confi_cat\":float})\n",
    "    #create promoted, promoted content table (ad information) with its' categories and topics\n",
    "    promoted = promoted.merge(topics_categories, how='left', on='document_id')\n",
    "    promoted.rename(columns={'document_id': 'ad_document_id'}, inplace=True)\n",
    "    \n",
    "    #assign nulls to -1 and cast the ids back to int (the merge changes them to float)\n",
    "    promoted[['confi_cat', 'confi_top']] = promoted[['confi_cat', 'confi_top']].fillna(0)\n",
    "    promoted = promoted.fillna(-1)\n",
    "    promoted[['topic_id', 'category_id']] = promoted[['topic_id', 'category_id']].astype(int)\n",
    "    \n",
    "    promoted.to_csv(path_b + 'promoted_content_prep.csv', index=False)\n",
    "    print \"promoted_content_prep.csv created in path/source_tables/built/ directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
